信息增益：
熵：表示随机变量的不确定性。
条件熵：在一个条件下，随机变量的不确定性。
信息增益：熵 - 条件熵
在一个条件下，信息不确定性减少的程度！通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， 
Y(明天阴天)也是随机变量，在阴天情况下下雨的信息熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。
两者相减就是信息增益！原来明天下雨例如信息熵是2，条件熵是0.01（因为如果是阴天就下雨的概率很大，信息就少了），这样相减后为1.99，在获得阴天这个信息后，
下雨信息不确定性减少了1.99！是很多的！所以信息增益大！也就是说，阴天这个信息对下雨来说是很重要的！所以在特征选择的时候常常用信息增益，
如果IG（信息增益大）的话那么这个特征对于分类来说很关键。


生成决策树：
1.对于每一个特征，找到一个使得Gini值最小的分割点（这个分割点可以是>,<,>=这样的判断，也可以是=，!=），然后比较每个特征之间最小的Gini值，作为当前最优的特征的最优分割点（这实际上涉及到了两个步骤，选择最优特征以及选择最优分割点）。

2.在第一步完成后，会生成两个叶节点，我们对这两个叶节点做判断，计算它的Gini值是否足够小（若是，就将其作为叶子不再分类）

3.将上步得到的叶节点作为新的集合，进行步骤1的分类，延伸出两个新的叶子节点（当然此时该节点变成了父节点）

4.循环迭代至不再有Gini值不符合标准的叶节点

GINI不纯度：
Gini不纯度是对分类结果好坏的度量标准
他的值是：1-每个标签占总数的比例的平方和。即1–∑mi=1fi2
对于上述的结果来讲，总的集合D被分为两个集合D1，D2，假设见面为1，不见面为0。
那么D1的不纯度为1-f1^2-f0^2，总数为5，见面的占了全部，则f1=1，f0=0，结果为0
D2的不纯度为1-f1^2-f0^2，f1=0.8，f0=0.2，结果为0.32
那么整个分类结果的Gini不纯度就是D1/D与0的乘积 加上 D2/D与0.32的乘积，为0.16
Gini值代表了某一个分类结果的“纯度”，我们希望结果的纯度很高，这样就不需要对这一结果进行处理了。
Gini值越小，纯度越高，结果越好。

随机森林：
鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，我们假设随机森林使用了m棵决策树，
那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，
对于模型的泛化能力是有害的

产生n个样本的方法采用Bootstraping法，这是一种有放回的抽样方法，产生n个样本

而最终结果采用Bagging的策略来获得，即多数投票机制

随机森林的生成方法：

1.从样本集中通过重采样的方式产生n个样本

2.假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点

3.重复m次，产生m棵决策树

4.多数投票机制来进行预测

（需要注意的一点是，这里m是指循环的次数，n是指样本的数目，n个样本构成训练的样本集，而m次循环中又会产生m个这样的样本集）

网格搜索：
